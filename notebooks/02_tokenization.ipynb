{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4b069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Alejo\\Analisis de Datos\\transformers-sentiment-analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado correctamente: clean_reviews_v2.csv\n",
      "                                        review     label\n",
      "0    i loved this product, it works perfectly.  positive\n",
      "1           this exceeded all my expectations!  positive\n",
      "2    amazing quality, totally worth the price.  positive\n",
      "3  i'm extremely satisfied with this purchase.  positive\n",
      "4         great experience, i would buy again.  positive\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 02 - TOKENIZATION\n",
    "# Author: Alejandro Enr√≠quez\n",
    "# Project: transformers-sentiment-analysis\n",
    "# ======================================================\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Import libraries\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "#import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Upload the file\n",
    "df = pd.read_csv(\"../data/processed/clean_reviews_v2.csv\")\n",
    "\n",
    "print(\"File uploaded successfully: clean_reviews_v2.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer cargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Load tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244a5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  3866,  ...,     0,     0,     0],\n",
       "        [  101,  2023, 14872,  ...,     0,     0,     0],\n",
       "        [  101,  6429,  3737,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2009,  7719,  ...,     0,     0,     0],\n",
       "        [  101,  2009,  1055,  ...,     0,     0,     0],\n",
       "        [  101,  2009,  1055,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. Tokenize all reviews\n",
    "# -----------------------------\n",
    "tokens = tokenizer(\n",
    "    list(df[\"review\"]),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f57f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 3866, 2023, 4031, 1010, 2009, 2573, 6669, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo = df[\"review\"].iloc[0]\n",
    "tokenizer(ejemplo, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ba954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens guardados como tokens.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(tokens, \"tokens.pt\")\n",
    "\n",
    "print(\"Tokens saved as tokens.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
